using StatsBase
using Random
using Distributions
using CairoMakie
Random.seed!(0) # fix the seed to have consistent simulations

# P(X âˆˆ Î¼ Â± 2Ïƒ) â‰ˆ 95% 
Î¼true = [0.0, 4.0, 8.0]


###### 1) Simultate formward model 1\3 âˆ‘_i N(Î¼[i],1)
Nobs = 1000 # number observations
function samplemixtue(N, Î¼)
    obs = zeros(N) # initialise
    kk = zeros(Int64, N) # initialise    
    for i âˆˆ 1:N
        k = rand(1:length(Î¼true)) 
        kk[i] = k
        obs[i] = randn() + Î¼[k]
    end
    return obs, kk
end
obs, kk = samplemixtue(Nobs, Î¼true)



struct Entropy
    M::Matrix{Float64} # allocation matrix (N x K)
    p::Vector{Float64} # entropy probabilities
end




function gibbs(x, z0, Î¼0, Niter; version = :standard)
    if version == :standard 
        println("Standard gibbs") 
    elseif version == :adapt 
        println("Entropy based gibbs")
    else
        error("This version has not been implemented")
    end
    n = length(obs)
    _M = probs!(zeros(length(obs), length(Î¼0)), Î¼0) # initialise allocation matrix
    _p = [1/n for i âˆˆ axes(_M, 1)] # initialise probabilities 
    E = Entropy(_M, _p) 
    zz = fill(zeros(Int64, length(obs)), Niter) # initilise container for output
    mm = fill(zeros(Float64, length(Î¼0)), Niter) # initilise container for output
    zz[1] = z0
    mm[1] = Î¼0
    z = copy(z0)
    Î¼ = copy(Î¼0)
    for i in 2:Niter
        z = gibbsZ!(E, x, z, Î¼, i, version)
        Î¼ = gibbsÎ¼!(x, z, Î¼)
        zz[i] = copy(z)
        mm[i] = copy(Î¼)
    end
    return zz, mm
end


function gibbsZ!(E::Entropy, x, z, Î¼, i, version)
    p = E.p
    if version == :adapt 
        (i%100 ==0  && i > 500) == 0
        E = update_p!(E, i)
    end
    j = sample(Weights(p))  # not efficient, see ?sample. For improving: see https://github.com/TuringLang/AdvancedPS.jl/blob/master/src/resampling.jl
    E = update_row!(E, j, x, Î¼)
    z[j] = sample(Weights(E.M[j,:]))
    return z
end

# check carefully the adaptation below
function update_p!(E::Entropy, i)  
    M = E.M
    E.p .= E.p.*(i - 1)/i/100 .+ (1/i)/100*(0.95*[entropy(M[i,:]) for i âˆˆ axes(M, 1)] .+ 0.05*1/length(E.p))
    return E
end

# TODO: work with log probabilities.
function update_row!(E, j, x, Î¼) # update allocation matrix 
    p = [exp(-(x[j] - Î¼i)^2/2) for Î¼i âˆˆ Î¼]  # should not need to normalise
    E.M[j, :] .= p 
    return E
end


function gibbsÎ¼!(x, z, Î¼) 
    for i âˆˆ eachindex(Î¼)
        ind = z .== i
        xind = x[ind]
        tot = sum(ind)
        Î¼[i] = randn()/sqrt(tot) + sum(xind)/tot   ### TO CHECK Î¼_i | z  = ð’©(xbar, 1/nbar) xbar = âˆ‘_j x_J 1(z_j = i) 
    end
    return Î¼
end

# true z prob under Î¼true
function probs!(Pz, Î¼)
    for i in eachindex(obs)
        pi = [exp(-(obs[i] - Î¼j)^2/2) for Î¼j in Î¼]
        ptot = sum(pi)
        pi ./= ptot # normalise
        Pz[i, :] .= pi 
    end
    Pz
end


function entropy(p)
    -sum(pi*log(pi) for pi in p)
end

function bayesfactors!(Cj, j, x, Î¼)   ### CHECK SMC TRICKS, LOOK AT SequentialMonteCarlo.jl
    p = [exp(-(x[j] - Î¼i)^2/2) for Î¼i in Î¼] 
    # p = p./sum(p)
    Cj.p .= p 
    return Cj
end

initialization = :random
if initialization == :random
    println("random allocation of cluster and random initialization of means")
    z0 = [rand(1:length(Î¼true)) for _ in eachindex(obs)]
    Î¼0 = randn(length(Î¼true)) 
elseif initialization == :true
    println("initialization from the true parameters")
    z0 = kk
    Î¼0 = Î¼true
else
    error("please specify initialization")
end
Niter = 100000
version =  :adapt 
# version = :standard
@time trace = gibbs(obs, z0, Î¼0, Niter, version = version)
zz, mm = trace


#### PLOTS AND DISPLAYES
display([[sum(getindex.(trace[2][end-1000:end], i))/1001 for i in 1:length(Î¼true)]  Î¼true ])
A = [[sum(getindex.(trace[2][end-1000:end], i))/1001 for i in 1:length(Î¼true)]  Î¼true] 
println("check accuracy: $(sum(A[:,1]) - sum(A[:,2]))")

# https://arxiv.org/abs/1708.05678
function jumpdistance(zz)
    hh = zeros(Int64, length(zz[1]))
    avjd = 0
    for i in eachindex(zz)[2:end]
        j0 = findfirst(x -> x !=  0, zz[i]- zz[i-1])
        if j0 â‰  nothing
            hh[j0] += 1
        end
        j = abs(sum(zz[i] - zz[i-1]))
        #  j > 2 && error("")
         if j == 0.0
            continue
         else
            avjd += 1
         end
    end
    avjd/length(zz), hh
end

avjd, hh = jumpdistance(zz)
println("Average squared jump distance: $(avjd)")



colobs = hh/maximum(hh) 
fig1 = Figure()
ax = Axis(fig1[1,1])
sct = scatter!(ax, obs, markersize = colobs*25.0 .+ 5.0,
color = colobs,
colormap = :thermal)
Colorbar(fig1[1, 2], sct)
hlines!(ax, Î¼true, color = :red, linestyle = :dash)
hlines!(ax, [(Î¼true[1] + Î¼true[2])/2, (Î¼true[2] + Î¼true[3])/2] , color = :blue, linestyle = :dash)
save("./version_$(version)_scatterplot.pdf", fig1)


fig2 = Figure()
ax2 = Axis(fig2[1,1])
for i in eachindex(mm[1])
    lines!(ax2, getindex.(mm, i))
end
hlines!(ax2, Î¼true, color = (:red, 0.1), linestyle = :dash)
save("./version_$(version)_trace_means.pdf", fig2)